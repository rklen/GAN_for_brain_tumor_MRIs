{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Import Libraries"
      ],
      "metadata": {
        "id": "o__RWPq4nUUl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, all required libraries are imported."
      ],
      "metadata": {
        "id": "I2RAq4GJBCvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import cv2\n",
        "import csv\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Flatten, Conv2D, Reshape, Input, Conv2DTranspose\n",
        "from keras.layers import Activation, LeakyReLU, BatchNormalization, Dropout, Resizing\n",
        "from keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Reshape, Conv2DTranspose, LeakyReLU\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import nibabel as nib\n",
        "import sklearn\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import scipy\n",
        "from scipy.stats import wilcoxon\n",
        "import imgaug.augmenters as iaa\n",
        "import pandas as pd\n",
        "from scipy.stats import mannwhitneyu\n"
      ],
      "metadata": {
        "id": "-baXiCqrnWwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import Dataset"
      ],
      "metadata": {
        "id": "LkyK4UUkoQr0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the required dataset and resize it to the required dimensions i.e. 64x64 (in this case)."
      ],
      "metadata": {
        "id": "mOLkJT8sBJnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAIN_DIR = \"trainDataPos\"\n",
        "\n",
        "def load_images(folder):\n",
        "\n",
        "    imgs = []\n",
        "    target = 1\n",
        "    labels = []\n",
        "    for i in os.listdir(folder):\n",
        "        img_dir = os.path.join(folder,i)\n",
        "        try:\n",
        "            img = cv2.imread(img_dir)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "            img = cv2.resize(img, (64 , 64))\n",
        "            imgs.append(img)\n",
        "            labels.append(target)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    imgs = np.array(imgs)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    return imgs, labels\n",
        "\n",
        "data, labels = load_images(MAIN_DIR)\n",
        "data.shape, labels.shape\n",
        "\n",
        "np.random.seed(SEED)\n",
        "idxs = np.random.randint(0, 1000, 1000)\n",
        "\n",
        "X_train = data[idxs]\n",
        "X_train.shape\n",
        "\n",
        "X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
        "\n",
        "X_train = X_train.reshape(-1, WIDTH,HEIGHT,CHANNELS)\n",
        "\n",
        "X_train.shape"
      ],
      "metadata": {
        "id": "LiQNhUY2oTux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Set the Parameters for the Generated Images\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gdZ7LvorCgFU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the dimensions of the noise vector, batch size during each iteration, number of epochs, number of steps per epoch, and seed, and set the dimensions of the image to be generate."
      ],
      "metadata": {
        "id": "l0Ij4pUYCFCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NOISE_DIM = 100\n",
        "BATCH_SIZE = 4\n",
        "STEPS_PER_EPOCH = 3750\n",
        "EPOCHS = 100\n",
        "SEED = 40\n",
        "WIDTH, HEIGHT, CHANNELS = 64, 64, 1\n",
        "\n",
        "OPTIMIZER = Adam(0.0002, 0.5)"
      ],
      "metadata": {
        "id": "iaSSDou5Bklj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot the images"
      ],
      "metadata": {
        "id": "zxV_io66oqDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,8))\n",
        "for i in range(10):\n",
        "    axs = plt.subplot(2,5,i+1)\n",
        "    plt.imshow(X_train[i], cmap=\"gray\")\n",
        "    plt.axis('off')\n",
        "    axs.set_xticklabels([])\n",
        "    axs.set_yticklabels([])\n",
        "    plt.subplots_adjust(wspace=None, hspace=None)\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "O3rcy3QxovJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility Function\n",
        "\n"
      ],
      "metadata": {
        "id": "5LDCQ3WvoxYF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generator Model"
      ],
      "metadata": {
        "id": "mvuaLKUNo3Is"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 'build_generator()' function constructs the generator part of the GAN model. Initilizes a sequential model, for stacking layers in sequence. The Generator starts with the dense layer with 8x8x512 neurons followed by LeakyReLU Activation. These neurons are upsampled through 4 transposed convolutional layers with filters added. Filters are progressively decreasing as 256, 128, 64, CHANNEL, each filter followed by LeakyReLU except for the last layer, which uses tanh. The model is complied by binary cross entropy and optimized by Adam optimizer."
      ],
      "metadata": {
        "id": "juUAJUEJDCS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_generator():\n",
        "    model = Sequential([\n",
        "        Dense(8*8*512, input_dim=NOISE_DIM, use_bias=False),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        Reshape((8, 8, 512)),\n",
        "\n",
        "        Conv2DTranspose(256, (4, 4), strides=(2, 2), padding='same', use_bias=False),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "\n",
        "        Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', use_bias=False),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "\n",
        "        Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same', use_bias=False),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "\n",
        "        Conv2DTranspose(CHANNELS, (4, 4), padding='same', activation='tanh')\n",
        "    ], name=\"generator\")\n",
        "\n",
        "    model.summary()\n",
        "    model.compile(loss=\"binary_crossentropy\", optimizer=OPTIMIZER)\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "CxOpTdLco9W9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discriminator Model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eVIRCyRWpA-5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 'build_discriminator()' function constructs the discriminator part of GAN using a sequential model. The discriminator uses 4 convolutional layers with filters increasing as 64,128,128,256 followed by LeakyReLU activation. There is a dense layer with a single neuron and sigmoid activation that is added to indicate whether an image is real or fake."
      ],
      "metadata": {
        "id": "9IhoXdTHE4ns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_discriminator():\n",
        "\n",
        "    model = Sequential([\n",
        "        Conv2D(64, (3, 3), padding='same', input_shape=(WIDTH, HEIGHT, CHANNELS)),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "\n",
        "        Conv2D(128, (3, 3), strides=2, padding='same'),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "\n",
        "        Conv2D(128, (3, 3), strides=2, padding='same'),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "\n",
        "        Conv2D(256, (3, 3), strides=2, padding='same'),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "\n",
        "        Flatten(),\n",
        "        Dropout(0.4),\n",
        "        Dense(1, activation=\"sigmoid\")\n",
        "    ], name=\"discriminator\")\n",
        "\n",
        "    model.summary()\n",
        "    model.compile(loss=\"binary_crossentropy\", optimizer=OPTIMIZER)\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "7F9pvon5pDqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Artifically Synthesise Images"
      ],
      "metadata": {
        "id": "DEnKDrCQpVby"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 'sample_image' function is called out to generate images from a noise input using the GAN's generator model and display them in grid format. Each generated image is saved in a specified directory. It subplots the image."
      ],
      "metadata": {
        "id": "EJp-ciwTHRc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_images(noise, subplots, figsize=(22,8), save=False):\n",
        "    generated_images = generator.predict(noise)\n",
        "    plt.figure(figsize=figsize)\n",
        "\n",
        "    for i, image in enumerate(generated_images):\n",
        "        plt.subplot(subplots[0], subplots[1], i+1)\n",
        "        if CHANNELS == 1:\n",
        "            plt.imshow(image.reshape((WIDTH, HEIGHT)), cmap='gray')\n",
        "\n",
        "        else:\n",
        "            plt.imshow(image.reshape((WIDTH, HEIGHT, CHANNELS)))\n",
        "        if save == True:\n",
        "            img_name = \"output64x64/gen\" + str(i)\n",
        "            plt.savefig(img_name)\n",
        "        plt.subplots_adjust(wspace=None, hspace=None)\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "vKE42BXLpYxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Start the GAN"
      ],
      "metadata": {
        "id": "nPT1uD91pNWl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make sure only generator is trained in the GAN model. A GAN input is defined, which is fed to generator to create a fake image. The fake image is then fed to discriminator."
      ],
      "metadata": {
        "id": "cXLd1KXOGK5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print('\\n')\n",
        "discriminator = build_discriminator()\n",
        "print('\\n')\n",
        "generator = build_generator()\n",
        "\n",
        "discriminator.trainable = False\n",
        "\n",
        "gan_input = Input(shape=(NOISE_DIM,))\n",
        "fake_image = generator(gan_input)\n",
        "\n",
        "gan_output = discriminator(fake_image)\n",
        "\n",
        "gan = Model(gan_input, gan_output, name=\"gan_model\")\n",
        "gan.compile(loss=\"binary_crossentropy\", optimizer=OPTIMIZER)\n",
        "\n",
        "print(\"The Combined Network:\\n\")\n",
        "gan.summary()"
      ],
      "metadata": {
        "id": "bPVJJ6eHpSTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Run the model"
      ],
      "metadata": {
        "id": "Gk_OfagPqAMf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start the tensroflow session and generate the images."
      ],
      "metadata": {
        "id": "owllTn0bIJL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(SEED)\n",
        "for epoch in range(10):\n",
        "    for batch in tqdm(range(STEPS_PER_EPOCH)):\n",
        "\n",
        "        noise = np.random.normal(0,1, size=(BATCH_SIZE, NOISE_DIM))\n",
        "        fake_X = generator.predict(noise)\n",
        "\n",
        "        idx = np.random.randint(0, X_train.shape[0], size=BATCH_SIZE)\n",
        "        real_X = X_train[idx]\n",
        "\n",
        "        X = np.concatenate((real_X, fake_X))\n",
        "\n",
        "        disc_y = np.zeros(2*BATCH_SIZE)\n",
        "        disc_y[:BATCH_SIZE] = 1\n",
        "\n",
        "        d_loss = discriminator.train_on_batch(X, disc_y)\n",
        "\n",
        "        y_gen = np.ones(BATCH_SIZE)\n",
        "        g_loss = gan.train_on_batch(noise, y_gen)\n",
        "\n",
        "    print(f\"EPOCH: {epoch + 1} Generator Loss: {g_loss:.4f} Discriminator Loss: {d_loss:.4f}\")\n",
        "    noise = np.random.normal(0, 1, size=(10,NOISE_DIM))\n",
        "    sample_images(noise, (2,5))"
      ],
      "metadata": {
        "id": "la-lNEieqCXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predict the image"
      ],
      "metadata": {
        "id": "FNC4NeWlqGRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generated_images = generator.predict(noise)\n",
        "generated_images.shape"
      ],
      "metadata": {
        "id": "Zt2QUjqwqF8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate Sample Images"
      ],
      "metadata": {
        "id": "FyM86EcpqLbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "noise = np.random.normal(0, 1, size=(100, NOISE_DIM))\n",
        "sample_images(noise, (10,10), (24,20), save=True)"
      ],
      "metadata": {
        "id": "9TLM_5f_qNn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Plot comparing the distribution of real and generated images."
      ],
      "metadata": {
        "id": "fT3osLgNqTTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(ncols=1, nrows=1, figsize=(18,10))\n",
        "\n",
        "sns.distplot(X_train, label='Real Images', hist=True, color='#fc0328', ax=axs)\n",
        "sns.distplot(generated_images, label='Generated Images', hist=True, color='#0c06c7', ax=axs)\n",
        "\n",
        "axs.legend(loc='upper right', prop={'size': 12})\n",
        "\n",
        "output_path = 'GANplot100.png'\n",
        "plt.savefig(output_path, bbox_inches='tight')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Xs2n3GEnqcMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the Generated Images"
      ],
      "metadata": {
        "id": "hfx3we3pqgoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_images(images, path, start_index=0):\n",
        "    \"\"\"\n",
        "    Saves a batch of images to the specified directory.\n",
        "\n",
        "    Parameters:\n",
        "    - images: A batch of images as a NumPy array.\n",
        "    - path: The directory where images will be saved.\n",
        "    - start_index: The starting index for naming saved image files.\n",
        "    \"\"\"\n",
        "    for i, img_array in enumerate(images):\n",
        "\n",
        "        plt.imshow(img_array.reshape((WIDTH, HEIGHT)), cmap='gray')\n",
        "\n",
        "        filename = f\"{path}/GANpos{start_index + i}.png\"\n",
        "\n",
        "        plt.savefig(filename)\n",
        "\n",
        "noise = np.random.normal(0, 1, size=(1000,NOISE_DIM))\n",
        "generated_images = generator.predict(noise)\n",
        "print(generated_images.shape)\n",
        "save_images(generated_images, 'GANpos')\n"
      ],
      "metadata": {
        "id": "MajC-1dXqjHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating the results"
      ],
      "metadata": {
        "id": "wkJqpZQCqsPN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the Training Data"
      ],
      "metadata": {
        "id": "kfQlsNP2q41B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pathToTrainNeg='trainDataNeg'\n",
        "pathToTrainPos='trainDataPos'\n",
        "pathToTestNeg='testSetNeg'\n",
        "pathToTestPos='testSetPos'\n",
        "\n",
        "trainDataSize=10\n",
        "testSetSize=1000\n",
        "\n",
        "img_height = 64\n",
        "\n",
        "first50= pathToTrainNeg[:5]\n",
        "first50= pathToTrainPos[:5]\n",
        "\n"
      ],
      "metadata": {
        "id": "4dMQvuvPq6kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the Generated Data"
      ],
      "metadata": {
        "id": "oPgtKeHBrCrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doUWantAugmentation='yes' #yes or no\n",
        "numberOfAugmentedImages=1000\n",
        "\n",
        "pathToGANNeg= 'GANneg'\n",
        "pathToGANPos= 'GANpos'\n",
        "\n",
        "first50= pathToGANNeg[:500]\n",
        "first50= pathToGANPos[:500]"
      ],
      "metadata": {
        "id": "6YDVopMwrGPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the x_train and y_train to store training and test data."
      ],
      "metadata": {
        "id": "TXkoc3eLrPTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train=[]\n",
        "y_train=[]\n",
        "x_test=[]\n",
        "y_test=[]"
      ],
      "metadata": {
        "id": "XokbOFt-rTMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensuring equal number of positive and negative samples in training data."
      ],
      "metadata": {
        "id": "SpTrL2h1rXXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(int(trainDataSize/2)):\n",
        "    img_path='{}/{}'.format(pathToTrainNeg,os.listdir(pathToTrainNeg)[i])\n",
        "    img=cv2.imread(img_path)\n",
        "    img=cv2.resize(img[:,:,0],(img_height,img_height))\n",
        "    x_train.append(img)\n",
        "    y_train.append(0)\n",
        "    img_path1='{}/{}'.format(pathToTrainPos,os.listdir(pathToTrainPos)[i])\n",
        "    img1=cv2.imread(img_path1)\n",
        "    img1=cv2.resize(img1[:,:,0],(img_height,img_height))\n",
        "    x_train.append(img1)\n",
        "    y_train.append(1)"
      ],
      "metadata": {
        "id": "DCogzZYErVID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " To check if model need to be tested with augmented images (GAN generated images) or not."
      ],
      "metadata": {
        "id": "cZsmfv4N8uvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if(doUWantAugmentation=='yes'):\n",
        "    for i in range(int(numberOfAugmentedImages/2)):\n",
        "        img_path='{}/{}'.format(pathToGANNeg,os.listdir(pathToGANNeg)[i])\n",
        "        img=cv2.imread(img_path)\n",
        "        img=cv2.resize(img[:,:,0],(img_height,img_height))\n",
        "        x_train.append(img)\n",
        "        y_train.append(0)\n",
        "        img_path1='{}/{}'.format(pathToGANPos,os.listdir(pathToGANPos)[i])\n",
        "        img1=cv2.imread(img_path1)\n",
        "        img1=cv2.resize(img1[:,:,0],(img_height,img_height))\n",
        "        x_train.append(img1)\n",
        "        y_train.append(1)"
      ],
      "metadata": {
        "id": "f95Jd-lMrfJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Load Positive and negative images and convert them into arrays."
      ],
      "metadata": {
        "id": "YPUzYj0sri3n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load test images from directory to populate the 'x_test' and 'y_test'. Images are taken from both positive and negative samples. Process these images to make a array and print the length of these arrays."
      ],
      "metadata": {
        "id": "kUiLsbASIqZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(int(testSetSize/2)):\n",
        "    img_path='{}/{}'.format(pathToTestNeg,os.listdir(pathToTestNeg)[i])\n",
        "    img=cv2.imread(img_path)\n",
        "    img=cv2.resize(img[:,:,0],(img_height,img_height))\n",
        "    x_test.append(img)\n",
        "    y_test.append(0)\n",
        "    img_path1='{}/{}'.format(pathToTestPos,os.listdir(pathToTestPos)[i])\n",
        "    img1=cv2.imread(img_path1)\n",
        "    img1=cv2.resize(img1[:,:,0],(img_height,img_height))\n",
        "    x_test.append(img1)\n",
        "    y_test.append(1)\n",
        "\n",
        "x_train=np.array(x_train)\n",
        "y_train=np.array(y_train)\n",
        "x_test=np.array(x_test)\n",
        "y_test=np.array(y_test)\n",
        "\n",
        "\n",
        "print(len(x_train))\n",
        "print(len(y_train))\n",
        "print(len(x_test))\n",
        "print(len(y_test))"
      ],
      "metadata": {
        "id": "WKhySg1xrkSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot one image from the data"
      ],
      "metadata": {
        "id": "X65MmA3OroEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i=8\n",
        "print(y_train[i])\n",
        "plt.imshow(x_train[i],cmap='gray')"
      ],
      "metadata": {
        "id": "nX6TNtTHrrGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PredictwithUnet"
      ],
      "metadata": {
        "id": "6yJwG2uHrsfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predictWithUnet(x_train, y_train, x_test, numberOfEpochs):\n",
        "    img_height = x_train[0].shape[0]\n",
        "    model = Sequential([\n",
        "        Conv2D(64, 3, padding='same', input_shape=(img_height, img_height, 1)),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        BatchNormalization(),\n",
        "        Conv2D(64, 3, padding='same'),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
        "\n",
        "        Conv2D(128, 3, padding='same'),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        BatchNormalization(),\n",
        "        Conv2D(128, 3, padding='same'),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(strides=(2, 2)),\n",
        "\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dense(64, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "        metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
        "    )\n",
        "\n",
        "    earlystopping = EarlyStopping(monitor='val_loss', mode='min', patience=15, restore_best_weights=True)\n",
        "\n",
        "    history = model.fit(x=x_train, y=y_train, epochs=numberOfEpochs, validation_split=0.3, callbacks=[earlystopping], shuffle=True)\n",
        "    predictions = model.predict(x_test)\n",
        "    trainPreds = model.predict(x_train)\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.plot(history.history['loss'], color='blue')\n",
        "    plt.title('Model Loss Progression')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.show()\n",
        "\n",
        "    return [predictions, trainPreds]\n"
      ],
      "metadata": {
        "id": "gNm1kz-lrzjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## InceptionV3"
      ],
      "metadata": {
        "id": "u7Pr9brvr1_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predictWithInception(x_train,y_train,x_test,numEpochs):\n",
        "\n",
        "    img_height=x_train[0].shape[0]\n",
        "    if len(x_train.shape)==3:\n",
        "        x_train=np.rollaxis(np.array([x_train,x_train,x_train]),0,4)\n",
        "        x_test=np.rollaxis(np.array([x_test,x_test,x_test]),0,4)\n",
        "    ntf_model = keras.applications.InceptionV3(weights=None,input_shape=(img_height,img_height,3),include_top=False)\n",
        "    ntf_model.trainable = True\n",
        "    inputs = keras.Input(shape=(img_height,img_height,3))\n",
        "    x = ntf_model(inputs, training=True)\n",
        "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "    outputs = keras.layers.Dense(1)(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer=keras.optimizers.SGD(1e-3),\n",
        "                loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "                metrics=[keras.metrics.BinaryAccuracy()])\n",
        "    model.fit(x=x_train, y=y_train, epochs=numEpochs, validation_split=0.3, shuffle=True)\n",
        "    ntfPredictions=model.predict(x_test)\n",
        "    ntfTrainPreds=model.predict(x_train)\n",
        "    return([ntfPredictions, ntfTrainPreds])"
      ],
      "metadata": {
        "id": "pA3CR46Nr3Wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function for deciding the Threshold"
      ],
      "metadata": {
        "id": "XMkzUSS-sChg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def findThreshold(trainPredictions):\n",
        "\n",
        "    y_train=np.array([0,1]*int(trainPredictions.shape[0]/2))\n",
        "    fpr, tpr, thresholds = roc_curve(y_train, trainPredictions, drop_intermediate=False)\n",
        "    J_stats = tpr - fpr\n",
        "    youdensIndex = thresholds[np.argmax(J_stats)]\n",
        "    return([youdensIndex])"
      ],
      "metadata": {
        "id": "fOorACFssGf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function for evaluating the test set prediction"
      ],
      "metadata": {
        "id": "NRTaKWs9sJJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluatePredictions(testPredictions,threshold):\n",
        "\n",
        "    y_test=np.array([0,1]*int(testPredictions.shape[0]/2))\n",
        "\n",
        "    TN = 0\n",
        "    FN = 0\n",
        "    TP = 0\n",
        "    FP = 0\n",
        "\n",
        "    for i in range(len(y_test)):\n",
        "        if testPredictions[i]<threshold:\n",
        "            if y_test[i]==0:\n",
        "                TN+=1\n",
        "            else:\n",
        "                FN+=1\n",
        "        else:\n",
        "            if y_test[i]==1:\n",
        "                TP+=1\n",
        "            else:\n",
        "                FP+=1\n",
        "\n",
        "    acc = (TN+TP)/(TN+FN+TP+FP)\n",
        "    sen = TP/(FN+TP)\n",
        "    spe = TN/(TN+FP)\n",
        "    f1 = TP/(TP+1/2*(FP+FN))\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, testPredictions, drop_intermediate=False)\n",
        "    auc_value=auc(fpr, tpr)\n",
        "\n",
        "    return([acc,sen,spe,f1,auc_value])"
      ],
      "metadata": {
        "id": "7eNKYYKusI1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the Unet"
      ],
      "metadata": {
        "id": "CYQPSFa9sP2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numberOfEpochs=100\n",
        "k=0\n",
        "\n",
        "testPredictions,trainPredictions=predictWithUnet(x_train,y_train,x_test,numberOfEpochs)\n",
        "threshold=findThreshold(trainPredictions)\n",
        "acc,sen,spe,f1,auc_value=evaluatePredictions(testPredictions,threshold)\n",
        "\n",
        "print('Accuracy:',acc)\n",
        "print('Sensitivity:',sen)\n",
        "print('Specificity:',spe)\n",
        "print('F1 score:',f1)\n",
        "print('AUC:',auc_value)"
      ],
      "metadata": {
        "id": "k8WZAGyVsRti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save results and write result to csv file"
      ],
      "metadata": {
        "id": "hYcyjaPCsU78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_dir = 'resultsGAN'\n",
        "\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "results_file = os.path.join(results_dir, 'results10.1000.csv')\n",
        "\n",
        "results = []\n",
        "\n",
        "for j in range(20):\n",
        "    testPredictions, trainPredictions = predictWithUnet(x_train, y_train, x_test, numberOfEpochs)\n",
        "    threshold = findThreshold(trainPredictions)\n",
        "    acc, sen, spe, f1, auc_value = evaluatePredictions(testPredictions, threshold)\n",
        "\n",
        "    results.append([acc])\n",
        "\n",
        "with open(results_file, 'w', newline='') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile)\n",
        "\n",
        "    csvwriter.writerow(['Accuracy'])\n",
        "\n",
        "    csvwriter.writerows(results)\n"
      ],
      "metadata": {
        "id": "WdmvHaFtsbEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate mean accuracy with standard deviation"
      ],
      "metadata": {
        "id": "d78l4nNcsnQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mean_and_std_from_csv(file_path):\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "       values = df.iloc[:, 0].tolist()\n",
        "\n",
        "        if values:\n",
        "            mean_value = sum(values) / len(values)\n",
        "            std_deviation = (sum((x - mean_value) ** 2 for x in values) / len(values)) ** 0.5\n",
        "            return mean_value, std_deviation\n",
        "        else:\n",
        "            return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {file_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "file_path = 'resultsGAN/results10.1000.csv'\n",
        "mean_value, std_deviation = calculate_mean_and_std_from_csv(file_path)\n",
        "\n",
        "if mean_value is not None and std_deviation is not None:\n",
        "    print(f\"The mean value of the numbers in the file is: {mean_value}\")\n",
        "    print(f\"The standard deviation of the numbers in the file is: {std_deviation}\")\n",
        "else:\n",
        "    print(\"No valid numerical values found in the file.\")\n"
      ],
      "metadata": {
        "id": "7BE55KmKsxvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the Wilcoxon test"
      ],
      "metadata": {
        "id": "9uYI4aa9tBSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder_x = '912'\n",
        "folder_y = '911'\n",
        "\n",
        "def read_data_from_folder(folder):\n",
        "    data = []\n",
        "    for filename in os.listdir(folder):\n",
        "        filepath = os.path.join(folder, filename)\n",
        "        if os.path.isfile(filepath):\n",
        "            with open(filepath, 'r') as file:\n",
        "                for line in file:\n",
        "                    try:\n",
        "                        data.append(float(line.strip()))\n",
        "                    except ValueError:\n",
        "                        continue\n",
        "    return data\n",
        "\n",
        "x_values = read_data_from_folder(folder_x)\n",
        "y_values = read_data_from_folder(folder_y)\n",
        "\n",
        "stat, p_value = wilcoxon(x_values, y_values)\n",
        "\n",
        "print(f'Wilcoxon test statistic: {stat}')\n",
        "print(f'p-value: {p_value}')\n",
        "\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"There is a statistically significant difference between x and y (reject the null hypothesis).\")\n",
        "else:\n",
        "    print(\"There is no statistically significant difference between x and y (fail to reject the null hypothesis).\")\n"
      ],
      "metadata": {
        "id": "dUO6X46YtECe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the Mann-Whitney U test"
      ],
      "metadata": {
        "id": "NC5g3d8QQAjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "group1 = pd.read_csv('resultsGAN/results10.0.csv').squeeze().values\n",
        "group2 = pd.read_csv('resultsGAN/results10.10.csv').squeeze().values\n",
        "\n",
        "stat, p_value = mannwhitneyu(group1 , group2)\n",
        "print('Statistics=%.2f, p=%.2f' % (stat, p_value))\n",
        "\n",
        "alpha = 0.05\n",
        "\n",
        "if p_value < alpha:\n",
        "    print('Reject Null Hypothesis (Significant difference between two samples)')\n",
        "else:\n",
        "    print('Do not Reject Null Hypothesis (No significant difference between two samples)')"
      ],
      "metadata": {
        "id": "hYYcbqkEQFN8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}